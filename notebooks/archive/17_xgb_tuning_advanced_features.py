import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import gc

# ==========================================
# 1. –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–ù–ò–•
# ==========================================
DATA_PATH = 'data/processed/all_data_compressed.parquet'
print("üìÇ Loading data...")

df = pd.read_parquet(DATA_PATH)
df = df.sort_values('timestamp').reset_index(drop=True)

if not np.issubdtype(df['timestamp'].dtype, np.datetime64):
    df['timestamp'] = pd.to_datetime(df['timestamp'])

# ==========================================
# 2. FEATURE ENGINEERING
# ==========================================
print("üõ†Ô∏è Engineering features...")

# Target
SAFE_LIMIT, FAIL_LIMIT = 5000, 50000
df['degradation_score'] = ((df['hAcc'] - SAFE_LIMIT) / (FAIL_LIMIT - SAFE_LIMIT)).clip(0.0, 1.0)

# Basic Physics
if 'numSV' in df.columns and 'numSatsTracked' in df.columns:
    df['sat_efficiency'] = df['numSV'] / df['numSatsTracked'].replace(0, 1)
    df['sat_efficiency'] = df['sat_efficiency'].clip(0, 5)
else:
    df['sat_efficiency'] = 0.0

# Rolling Features (–∫–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ –≤ float32 –æ–¥—Ä–∞–∑—É)
df = df.set_index('timestamp')
rolling_windows = ['5s', '10s']
cols_to_roll = ['cnoMean', 'sat_efficiency']

for col in cols_to_roll:
    if col in df.columns:
        for w in rolling_windows:
            df[f'{col}_rolling_mean_{w}'] = df[col].rolling(w).mean().astype(np.float32)
            df[f'{col}_rolling_std_{w}'] = df[col].rolling(w).std().fillna(0).astype(np.float32)

df = df.reset_index()

# Lags
for col in ['cnoMean', 'sat_efficiency']:
    if col in df.columns:
        df[f'{col}_lag1'] = df[col].shift(1).bfill().astype(np.float32)

features = ['cnoMean', 'cnoStd', 'numSV', 'numSatsTracked', 'sat_efficiency'] + \
           [c for c in df.columns if 'rolling' in c] + \
           [c for c in df.columns if 'lag' in c]
features = [f for f in features if f in df.columns]

# ==========================================
# 3. –ß–ò–°–¢–ö–ê –ü–ê–ú'–Ø–¢–Ü
# ==========================================
print("üßπ Memory cleanup...")
cols_to_keep = features + ['degradation_score', 'timestamp']
df = df[cols_to_keep]

float_cols = df.select_dtypes(include=['float64']).columns
df[float_cols] = df[float_cols].astype(np.float32)
gc.collect()

# ==========================================
# 4. –†–û–ó–£–ú–ù–ï –†–û–ó–î–Ü–õ–ï–ù–ù–Ø (SMART SPLIT)
# ==========================================
print("‚úÇÔ∏è Splitting & Downsampling...")
split_date = pd.Timestamp('2025-12-01')

# –¢–ï–°–¢–û–í–ò–ô –Ω–∞–±—ñ—Ä –∑–∞–ª–∏—à–∞—î–º–æ –ü–û–í–ù–ò–ú (—â–æ–± —á–µ—Å–Ω–æ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏)
test_df = df[df['timestamp'] >= split_date].copy()

# –¢–†–ï–ù–£–í–ê–õ–¨–ù–ò–ô –Ω–∞–±—ñ—Ä —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ
train_full = df[df['timestamp'] < split_date]

# –°—Ç—Ä–∞—Ç–µ–≥—ñ—è: –ó–∞–ª–∏—à–∞—î–º–æ –≤—Å—ñ "–ê—Ç–∞–∫–∏" —ñ —Ç—ñ–ª—å–∫–∏ 25% "–°–ø–æ–∫–æ—é"
mask_attack = train_full['degradation_score'] > 0.05  # –í—Å—ñ –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ –ø–æ–¥—ñ—ó
mask_safe = train_full['degradation_score'] <= 0.05   # –°–ø–æ–∫—ñ–π

train_attack = train_full[mask_attack]
train_safe = train_full[mask_safe].sample(frac=0.25, random_state=42) # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ —á–≤–µ—Ä—Ç—å

# –û–±'—î–¥–Ω—É—î–º–æ –Ω–∞–∑–∞–¥
train_df = pd.concat([train_attack, train_safe]).sample(frac=1, random_state=42) # –ü–µ—Ä–µ–º—ñ—à—É—î–º–æ

print(f"   Original Train Size: {len(train_full)}")
print(f"   Optimized Train Size: {len(train_df)} (Memory Saved!)")

# –§–æ—Ä–º—É—î–º–æ X —Ç–∞ y
X_train = train_df[features]
y_train = train_df['degradation_score']
X_test = test_df[features]
y_test = test_df['degradation_score']

# –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤–µ
del df, train_full, train_attack, train_safe, train_df, test_df
gc.collect()

# ==========================================
# 5. –ù–ê–í–ß–ê–ù–ù–Ø (–¢—ñ–ª—å–∫–∏ –¢—é–Ω—ñ–Ω–≥–æ–≤–∞–Ω–∞)
# ==========================================
print("\nü§ñ Training Optimized Model...")

# –ú–∏ –Ω–∞–≤—á–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –æ–¥–Ω—É, –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å, —â–æ–± –µ–∫–æ–Ω–æ–º–∏—Ç–∏ —Ä–µ—Å—É—Ä—Å–∏
tuned_params = {
    'subsample': 0.7,
    'n_estimators': 100, 
    'max_depth': 6, 
    'learning_rate': 0.05, 
    'colsample_bytree': 0.7
}

model = XGBRegressor(
    objective='reg:logistic',
    n_jobs=-1,
    tree_method='hist', # –í–∞–∂–ª–∏–≤–æ!
    random_state=42,
    **tuned_params
)

model.fit(X_train, y_train)

print("‚úÖ Model Trained!")

# –ß–∏—Å—Ç–∏–º–æ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–æ–º
del X_train, y_train
gc.collect()

# ==========================================
# 6. –ë–ï–ó–ü–ï–ß–ù–ï –ü–ï–†–ï–î–ë–ê–ß–ï–ù–ù–Ø
# ==========================================
print("\nüîÆ Predicting in batches...")

def predict_in_batches(model, X, batch_size=500000):
    num_samples = len(X)
    predictions = []
    print(f"   Processing {num_samples} samples...")
    for start in range(0, num_samples, batch_size):
        end = min(start + batch_size, num_samples)
        batch_X = X.iloc[start:end]
        print(f"   Batch {start}-{end}...", end='\r')
        batch_pred = model.predict(batch_X)
        predictions.append(batch_pred)
        del batch_X, batch_pred
        gc.collect()
    print("\n   Done.")
    return np.concatenate(predictions)

y_pred = predict_in_batches(model, X_test)

# ==========================================
# 7. –û–¶–Ü–ù–ö–ê
# ==========================================
print("\nüìä Evaluation Results:")
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"MAE: {mae:.4f}")
print(f"RMSE: {rmse:.4f}")

# Feature Importance
plt.figure(figsize=(10, 8))
importance = pd.DataFrame({
    'Feature': features,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

sns.barplot(data=importance.head(15), x='Importance', y='Feature', palette='viridis')
plt.title('Top 15 Features (Final Model)')
plt.tight_layout()
plt.show()

# Zoom Plot
plt.figure(figsize=(12, 5))
subset = slice(10000, 10500)
plt.plot(y_test.iloc[subset].values, label='Actual', color='black', alpha=0.3)
plt.plot(y_pred[subset], label='Prediction', color='orange', linewidth=2)
plt.title('Prediction Sample')
plt.legend()
plt.show()